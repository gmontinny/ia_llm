# ü§ñ Introdu√ß√£o a Large Language Models (LLMs)

Aplica√ß√£o educacional para aprender, experimentar e construir com LLMs usando provedores como OpenAI, Hugging Face e Ollama. Inclui exemplos pr√°ticos e apps em Streamlit.


## üìã √çndice

- [Caracter√≠sticas](#caracter√≠sticas)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Requisitos](#requisitos)
- [Instala√ß√£o](#instala√ß√£o)
- [Configura√ß√£o](#configura√ß√£o)
- [Como Executar](#como-executar)
- [Exemplos de C√≥digo](#exemplos-de-c√≥digo)
- [Apps (Streamlit)](#apps-streamlit)
- [Dicas e Solu√ß√£o de Problemas](#dicas-e-solu√ß√£o-de-problemas)


## ‚ú® Caracter√≠sticas

- M√∫ltiplos provedores: OpenAI, Hugging Face (Inference API e pipeline local), Ollama
- Arquitetura modular e extens√≠vel (ModelFactory, BaseModel)
- Exemplos educativos cobrindo textos, chat com hist√≥rico, streaming e embeddings
- Apps em Streamlit prontos para uso
- Logging configurado e mensagens de erro √∫teis


## üìÅ Estrutura do Projeto

```text
ia_llm/
‚îú‚îÄ apps/
‚îÇ  ‚îî‚îÄ 01_chatbot_simples.py         # App Streamlit: chatbot b√°sico
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ examples/
‚îÇ  ‚îÇ  ‚îú‚îÄ 01_basic_completion.py     # Exemplo: conclus√£o b√°sica
‚îÇ  ‚îÇ  ‚îú‚îÄ 02_chat_with_history.py    # Exemplo: chat com hist√≥rico
‚îÇ  ‚îÇ  ‚îú‚îÄ 03_streaming_responses.py  # Exemplo: streaming de respostas
‚îÇ  ‚îÇ  ‚îú‚îÄ 04_embeddings.py           # Exemplo: embeddings e buscas
‚îÇ  ‚îÇ  ‚îî‚îÄ 05_comparing_models.py     # Exemplo: comparando modelos
‚îÇ  ‚îú‚îÄ models/
‚îÇ  ‚îÇ  ‚îú‚îÄ huggingface_model.py       # Integra√ß√£o com Hugging Face
‚îÇ  ‚îÇ  ‚îú‚îÄ ollama_model.py            # Integra√ß√£o com Ollama (local)
‚îÇ  ‚îÇ  ‚îî‚îÄ model_factory.py           # F√°brica de modelos
‚îÇ  ‚îî‚îÄ utils/                        # Utilidades e validadores
‚îú‚îÄ config/
‚îÇ  ‚îî‚îÄ settings.py                   # Configura√ß√µes e vari√°veis de ambiente
‚îú‚îÄ tests/                           # Testes automatizados (pytest)
‚îú‚îÄ requirements.txt                 # Depend√™ncias do projeto
‚îú‚îÄ README.md                        # Documenta√ß√£o do projeto
‚îî‚îÄ notebooks (.ipynb)               # Materiais do curso e projetos
```


## üß∞ Requisitos

- Python 3.10+
- Pip atualizados: `python -m pip install --upgrade pip`
- Para usar pipeline local da Hugging Face (transformers): √© necess√°rio PyTorch instalado
  - Windows (CPU): `pip install torch --index-url https://download.pytorch.org/whl/cpu`
  - Veja tamb√©m: https://pytorch.org/get-started/locally/
- Para Ollama: instalar e ter um modelo baixado (ex.: `ollama run phi3`)


## üöÄ Instala√ß√£o

1) Clone o reposit√≥rio e acesse a pasta

```bash
git clone <URL_DO_REPO>
cd ia_llm
```

2) Crie e ative um ambiente virtual

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

3) Instale as depend√™ncias

```bash
pip install -r requirements.txt
```


## üîß Configura√ß√£o

Defina as vari√°veis de ambiente conforme os provedores que voc√™ usar√°. Voc√™ pode criar um arquivo `.env` ou exportar no shell.

Vari√°veis comuns suportadas:
- OPENAI_API_KEY
- HUGGINGFACEHUB_API_TOKEN (preferencial) ou HUGGINGFACE_API_KEY
- OLLAMA_HOST (ex.: http://localhost:11434)

Algumas configura√ß√µes padr√£o podem ser alteradas no arquivo `config/settings.py`, por exemplo:
- DEFAULT_OPENAI_MODEL
- DEFAULT_HF_MODEL (padr√£o sugerido: google/flan-t5-base)
- DEFAULT_OLLAMA_MODEL
- DEFAULT_TEMPERATURE, DEFAULT_MAX_TOKENS
- DEFAULT_HF_BACKEND (endpoint ou pipeline)

Exemplo r√°pido (Windows PowerShell):
```powershell
$env:OPENAI_API_KEY = "sk-..."
$env:HUGGINGFACEHUB_API_TOKEN = "hf_..."
$env:OLLAMA_HOST = "http://localhost:11434"
```


## ‚ñ∂Ô∏è Como Executar

- Rodar um exemplo simples de completa√ß√£o:
```powershell
python src\examples\01_basic_completion.py
```

- Rodar o app Streamlit (chatbot simples):
```powershell
streamlit run apps\01_chatbot_simples.py
```

- Rodar outros exemplos:
```powershell
python src\examples\02_chat_with_history.py
python src\examples\03_streaming_responses.py
python src\examples\04_embeddings.py
python src\examples\05_comparing_models.py
```

- Executar teste r√°pido do pipeline HF (T5):
```powershell
python tests\teste_rapido.py
```


## üí° Exemplos de C√≥digo

A maneira recomendada √© usar o ModelFactory e ModelConfig.

- Completa√ß√£o b√°sica (generate):
```python
from src.models.model_factory import ModelFactory
from src.models.base_model import ModelConfig
from config.settings import settings

config = ModelConfig(
    model_name=settings.DEFAULT_HF_MODEL,  # ou DEFAULT_OPENAI_MODEL, DEFAULT_OLLAMA_MODEL
    temperature=0.7,
    max_tokens=256,
    additional_params={
        # Para HF: escolher backend explicitamente (opcional)
        # 'backend': 'pipeline'  # ou 'endpoint'
    }
)

llm = ModelFactory.create_model('huggingface', config)  # 'openai' | 'ollama'
texto = llm.generate("Explique brevemente o que √© um LLM.")
print(texto)
```

- Chat com hist√≥rico:
```python
from src.models.model_factory import ModelFactory
from src.models.base_model import ModelConfig
from config.settings import settings

config = ModelConfig(
    model_name=settings.DEFAULT_HF_MODEL,
    temperature=0.7,
    max_tokens=300
)
llm = ModelFactory.create_model('huggingface', config)

messages = [
    {"role": "user", "content": "Ol√°!"},
    {"role": "assistant", "content": "Ol√°! Como posso ajudar?"},
    {"role": "user", "content": "Resuma o conceito de embeddings em 2 frases."}
]
resposta = llm.chat(messages, system_message="Voc√™ √© um assistente √∫til e responde em portugu√™s.")
print(resposta)
```

- Streaming de respostas (quando suportado pelo backend/provedor):
```python
from src.models.model_factory import ModelFactory
from src.models.base_model import ModelConfig
from config.settings import settings

llm = ModelFactory.create_model(
    'huggingface',
    ModelConfig(model_name=settings.DEFAULT_HF_MODEL)
)

for chunk in llm.generate_stream("Liste 5 aplica√ß√µes de LLMs."):
    print(chunk, end="", flush=True)
```


## üß© Apps (Streamlit)

Esta pasta cont√©m aplica√ß√µes interativas constru√≠das com Streamlit para voc√™ experimentar os provedores de LLM do projeto.

- 01_chatbot_simples.py
  - O que √©: um chatbot b√°sico com hist√≥rico de conversa, sele√ß√£o de provedor (OpenAI, Hugging Face ou Ollama), controle de temperatura e limite de tokens.
  - Recursos:
    - Interface de chat usando st.chat_message
    - Hist√≥rico persistente na sess√£o
    - Mensagem de sistema configur√°vel (defini√ß√£o de comportamento do assistente)
    - M√©trica simples de mensagens enviadas
  - Como executar:
    - Windows PowerShell: `streamlit run apps\01_chatbot_simples.py`
    - Linux/Mac: `streamlit run apps/01_chatbot_simples.py`
  - Pr√©‚Äërequisitos/configura√ß√£o:
    - Defina as vari√°veis de ambiente dos provedores que deseja usar (ver se√ß√£o Configura√ß√£o)
    - Ajuste os modelos padr√£o em config/settings.py se necess√°rio
    - Para usar Hugging Face via pipeline local, instale PyTorch (ver Requisitos)
  - Personaliza√ß√£o r√°pida:
    - Altere a mensagem de sistema na sidebar para guiar o tom/respostas do assistente
    - Troque o provider na sidebar para comparar qualidade/lat√™ncia
    - Ajuste temperature e max_tokens conforme seu caso

Dica: Para criar novos apps, copie este arquivo como base, altere o t√≠tulo, os controles da sidebar e troque a l√≥gica de gera√ß√£o conforme o objetivo do app (por exemplo, usar embeddings, RAG, etc.).

## üÜò Dicas e Solu√ß√£o de Problemas

- Hugging Face Inference API sem token (401/403/404): o c√≥digo tenta automaticamente o backend `pipeline` para o mesmo modelo e, se configurado, um modelo de fallback. Configure HUGGINGFACEHUB_API_TOKEN para melhor experi√™ncia.
- Pipeline local (transformers) requer PyTorch. No Windows (CPU): `pip install torch --index-url https://download.pytorch.org/whl/cpu`.
- Se usar Ollama, verifique se o servi√ßo est√° ativo e se o modelo foi baixado previamente (`ollama run phi3`).
- Ajuste DEFAULT_HF_BACKEND para `pipeline` se quiser for√ßar execu√ß√£o local.
- Mensagens de erro incluem links √∫teis para o reposit√≥rio do modelo na Hugging Face.


---
Pronto! Explore os exemplos, rode o app e adapte para seus projetos.